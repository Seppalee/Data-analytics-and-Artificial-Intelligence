{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10 - Text classification with Tensorflow\n",
    "- In this exercise you utilize the *DisneylandReviews.csv* located in data_files directory.\n",
    "- This exercise has the following phases:\n",
    "    - Load the data from csv file.\n",
    "    - Create directory structure including sample files from the data you loaded.\n",
    "    - Train your neural network with the extracted data.\n",
    "    - Validate the operation of your trained model.\n",
    "- Use [this example](https://hantt.pages.labranet.jamk.fi/ttc2050-material/material/10-ai-text-classification-tensorflow/) as a reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Import all the necessary libraries listed in our Tensorflow example. Read the csv file DisneylandReviews.csv into a data structure of your choice (list, dict, json...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"data_files/DisneylandReviews.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Create the directory structure presented below either by using python's os library or manually. So there should be *disney_review_data* directory which has two subdirectories: *train* and *test*. Both of those directories should then have two subdirectories: *pos* and *neg*.\n",
    "\n",
    "```\n",
    "disney_review_data\n",
    "    |\n",
    "    |----train\n",
    "    |      |----pos\n",
    "    |      |----neg\n",
    "    |\n",
    "    |----test\n",
    "           |----pos\n",
    "           |----neg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Loop through your saved data and save it as text files (.txt) into the directory structure. First 80 % of the data should go into *pos* and *neg* subdirectories under the *train* directory with the following conditions:\n",
    "- pos = rating is 4 or more\n",
    "- neg = rating is 2 or less\n",
    "\n",
    "The last 20 % should go into the *pos* and *neg* subdirectories under the *test* directory using the same conditions as above. Rating value of 3 is considered to be neutral and should not be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = reviews.iloc[:34125]\n",
    "test_data = reviews.iloc[34125:]\n",
    "\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "test_data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "negative_train_data = train_data[(train_data[\"Rating\"] <= 2)]\n",
    "negative_train_data = negative_train_data[[\"Review_Text\"]]\n",
    "\n",
    "positive_train_data = train_data[(train_data[\"Rating\"] >= 4)]\n",
    "positive_train_data = positive_train_data[[\"Review_Text\"]]\n",
    "\n",
    "negative_test_data = test_data[(test_data[\"Rating\"] <= 2)]\n",
    "negative_test_data = negative_test_data[[\"Review_Text\"]]\n",
    "\n",
    "positive_test_data = test_data[(test_data[\"Rating\"] >= 4)]\n",
    "positive_test_data = positive_test_data[[\"Review_Text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for index, row in negative_train_data.iterrows():\n",
    "    if i > len(negative_train_data):\n",
    "        break\n",
    "    else:\n",
    "        f = open(\"data_files/disney_review_data/train/neg/\"+str(i)+\".txt\", \"w\")\n",
    "        f.write(row[0])\n",
    "        f.close()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "j = 0\n",
    "for index, row in positive_train_data.iterrows():\n",
    "    if j > len(positive_train_data):\n",
    "        break\n",
    "    else:\n",
    "        f = open(\"data_files/disney_review_data/train/pos/\"+str(j)+\".txt\", \"w\")\n",
    "        f.write(row[0])\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for index, row in negative_test_data.iterrows():\n",
    "    if k > len(negative_test_data):\n",
    "        break\n",
    "    else:\n",
    "        f = open(\"data_files/disney_review_data/test/neg/\"+str(k)+\".txt\", \"w\")\n",
    "        f.write(row[0])\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "for index, row in positive_test_data.iterrows():\n",
    "    if n > len(positive_test_data):\n",
    "        break\n",
    "    else:\n",
    "        f = open(\"data_files/disney_review_data/test/pos/\"+str(n)+\".txt\", \"w\")\n",
    "        f.write(row[0])\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Use material page linked above as a reference and implement the text classification example to your notebook. Now modify it so that your Disneyland review data will be read from the directory structure you created earlier. Run the notebook and ensure that no errors are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_split = 0.2\n",
    "seed = 42\n",
    "dataset_dir = 'aclImdb'\n",
    "max_features = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30380 files belonging to 2 classes.\n",
      "Using 24304 files for training.\n"
     ]
    }
   ],
   "source": [
    "raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory('data_files/disney_review_data/train', batch_size=batch_size, validation_split=validation_split, subset=\"training\", seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30380 files belonging to 2 classes.\n",
      "Using 6076 files for validation.\n"
     ]
    }
   ],
   "source": [
    "raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory('data_files/disney_review_data/train', batch_size=batch_size, validation_split=validation_split, subset='validation', seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7167 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory('data_files/disney_review_data/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,'[%s]' % re.escape(string.punctuation),'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160016    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,033\n",
      "Trainable params: 160,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, 16),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(1)])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "760/760 [==============================] - 27s 35ms/step - loss: 0.3222 - binary_accuracy: 0.9201 - val_loss: 0.2500 - val_binary_accuracy: 0.9218\n",
      "Epoch 2/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.2507 - binary_accuracy: 0.9203 - val_loss: 0.2362 - val_binary_accuracy: 0.9218\n",
      "Epoch 3/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.2351 - binary_accuracy: 0.9205 - val_loss: 0.2199 - val_binary_accuracy: 0.9231\n",
      "Epoch 4/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.2144 - binary_accuracy: 0.9245 - val_loss: 0.2001 - val_binary_accuracy: 0.9297\n",
      "Epoch 5/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1918 - binary_accuracy: 0.9313 - val_loss: 0.1797 - val_binary_accuracy: 0.9347\n",
      "Epoch 6/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1698 - binary_accuracy: 0.9375 - val_loss: 0.1623 - val_binary_accuracy: 0.9394\n",
      "Epoch 7/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1512 - binary_accuracy: 0.9435 - val_loss: 0.1487 - val_binary_accuracy: 0.9429\n",
      "Epoch 8/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1377 - binary_accuracy: 0.9483 - val_loss: 0.1386 - val_binary_accuracy: 0.9455\n",
      "Epoch 9/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1258 - binary_accuracy: 0.9530 - val_loss: 0.1312 - val_binary_accuracy: 0.9490\n",
      "Epoch 10/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1162 - binary_accuracy: 0.9561 - val_loss: 0.1258 - val_binary_accuracy: 0.9510\n",
      "Epoch 11/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1091 - binary_accuracy: 0.9589 - val_loss: 0.1216 - val_binary_accuracy: 0.9523\n",
      "Epoch 12/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.1024 - binary_accuracy: 0.9617 - val_loss: 0.1186 - val_binary_accuracy: 0.9533\n",
      "Epoch 13/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.0963 - binary_accuracy: 0.9638 - val_loss: 0.1165 - val_binary_accuracy: 0.9556\n",
      "Epoch 14/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.0925 - binary_accuracy: 0.9656 - val_loss: 0.1152 - val_binary_accuracy: 0.9564\n",
      "Epoch 15/15\n",
      "760/760 [==============================] - 12s 16ms/step - loss: 0.0873 - binary_accuracy: 0.9680 - val_loss: 0.1138 - val_binary_accuracy: 0.9562\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, validation_data=val_ds, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 20s 91ms/step - loss: 0.1701 - binary_accuracy: 0.9340\n",
      "Loss:  0.1701088398694992\n",
      "Accuracy:  0.9340030550956726\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224/224 [==============================] - 3s 12ms/step - loss: 0.1701 - accuracy: 0.9340\n",
      "0.9340030550956726\n"
     ]
    }
   ],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Create some test data to verify that your model works and present the prediction results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((None, 250), (None,)), types: (tf.int64, tf.int32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.014],\n",
       "       [0.884],\n",
       "       [0.002],\n",
       "       [0.259],\n",
       "       [0.582],\n",
       "       [0.64 ],\n",
       "       [0.019],\n",
       "       [0.002],\n",
       "       [0.978],\n",
       "       [0.767],\n",
       "       [0.054],\n",
       "       [0.019],\n",
       "       [0.098],\n",
       "       [0.   ],\n",
       "       [0.068],\n",
       "       [0.422],\n",
       "       [0.682],\n",
       "       [0.023],\n",
       "       [0.018],\n",
       "       [0.002]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = export_model.predict(test_reviews)\n",
    "\n",
    "results = np.around(results,3)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
